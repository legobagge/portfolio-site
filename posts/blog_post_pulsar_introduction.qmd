---
title: "Getting Started with Apache Pulsar: A Postman's Guide to Modern Message Streaming"
description: "A practical introduction to distributed messaging for developers‚Äîunderstand Pulsar through familiar analogies and build a complete real-time analytics pipeline"
author: "Lucas Bagge"
date: "2025-12-10"
categories: [Apache Pulsar, Streaming, Data Engineering, Tutorial]
highlight-style: monokai
draft: false
---

## Introduction: Why Pulsar?

Imagine you're building a modern application that needs to handle thousands of events per second‚Äîuser clicks, sensor readings, financial transactions, or social media updates. How do you ensure these messages get delivered reliably, in order, and without data loss? Enter **Apache Pulsar**, a cloud-native distributed messaging and streaming platform that's been quietly powering some of the world's largest applications.

Unlike traditional message queues that force you to choose between streaming or queuing semantics, Pulsar gives you both. It's like having a postal service that can handle everything from urgent telegrams to bulk mail distribution‚Äîall in one unified platform.

In this guide, we'll explore Pulsar through a familiar analogy, set up a complete working demo, and show you how to process real-time data with PySpark. Whether you're a backend developer looking to improve your architecture or a data engineer exploring streaming platforms, this hands-on introduction will get you up and running quickly.

------------------------------------------------------------------------

## Understanding Pulsar: Key Terminology

Before we dive in, let's understand the fundamental concepts that make Pulsar tick:

### **Topics**

A topic is a named channel for transmitting messages. Think of it as a specific mailbox address where messages are sent and received. Topics in Pulsar follow a hierarchical naming structure:

```         
persistent://tenant/namespace/topic-name
```

For example: `persistent://public/default/sales-events`

### **Producers**

Producers are applications that publish messages to topics. They're the senders in our messaging system‚Äîcreating and dispatching messages to their designated topics.

### **Consumers**

Consumers are applications that subscribe to topics and process incoming messages. They can work independently or as part of consumer groups for load balancing.

### **Subscriptions**

A subscription is a named configuration that determines how messages are delivered to consumers. Pulsar supports multiple subscription types:

-   **Exclusive**: Only one consumer can subscribe (think: single delivery person for a route)
-   **Shared**: Multiple consumers share the load (like multiple delivery trucks)
-   **Failover**: One active consumer with backups ready to take over
-   **Key_Shared**: Messages with the same key go to the same consumer

### **Messages**

The actual data being transmitted. Each message contains: - **Payload**: The actual data (can be bytes, JSON, Avro, etc.) - **Properties**: Key-value metadata - **Message ID**: Unique identifier - **Timestamp**: When the message was published

### **Brokers**

The Pulsar servers that handle message routing, storage, and delivery. They're the post offices in our analogy.

### **BookKeeper**

Pulsar's storage layer that ensures messages are durably persisted. Even if brokers fail, your messages are safe.

------------------------------------------------------------------------

## The Postman Analogy: Understanding Message Flow

Let's make Pulsar more intuitive with a postal service analogy:

### **Pulsar = The Postal Service**

Imagine Pulsar as a modern, highly efficient postal service:

**Topics = Mailbox Addresses** Just as each house has a unique address, each topic has a unique name. When you want to send mail about "sales events," you address it to the `sales-events` mailbox.

**Producers = Senders** Anyone can write a letter (create a message) and drop it in a mailbox (publish to a topic). The sender doesn't need to know who will read it‚Äîthey just need to know the correct address.

**Consumers = Recipients** People subscribe to mailboxes to receive mail. Multiple people can subscribe to the same mailbox (shared subscription), or you might have one primary recipient with a backup (failover subscription).

**Subscriptions = Delivery Preferences** When you subscribe to a topic, you specify how you want messages delivered: - **Exclusive**: Like a personal PO box‚Äîonly you can access it - **Shared**: Like a business address where any employee can pick up mail - **Failover**: Primary recipient with a backup person - **Key_Shared**: Mail sorted by sender goes to designated recipients

**Brokers = Post Offices** The postal service infrastructure that routes and handles delivery. If one post office closes, others take over the load.

**BookKeeper = Storage Facility** Even if the post office burns down, your letters are safely stored in a separate, fireproof facility. Messages persist until you're ready to process them.

### **Key Differences from Traditional Mail:**

1.  **Speed**: Messages arrive in milliseconds, not days
2.  **Ordering**: Messages arrive in the exact order they were sent
3.  **Reliability**: Automatic redelivery if a consumer fails to acknowledge
4.  **Scalability**: Can handle millions of messages per second
5.  **Multi-tenancy**: Multiple organizations can share the same infrastructure securely

------------------------------------------------------------------------

## Demo: Building a Real-Time Sales Analytics Pipeline

Now let's put theory into practice. We'll build a complete system that: 1. Generates dummy sales transactions 2. Streams them through Pulsar 3. Processes and aggregates data with PySpark 4. Visualizes insights in real-time

### **Architecture Overview**

```         
Data Generator ‚Üí Pulsar Topic ‚Üí PySpark Consumer ‚Üí Analytics & Visualization
   (Python)     (sales-events)    (in Docker)         (Jupyter)
```

### **Prerequisites**

-   Docker & Docker Compose
-   Python 3.8+
-   Basic familiarity with command line

### **Step 1: Set Up the Infrastructure**

First, create a `docker-compose.yml` file:

``` yaml
version: '3.8'

services:
  pulsar:
    image: apachepulsar/pulsar:4.1.2
    container_name: pulsar-standalone
    ports:
      - "6650:6650"  # Pulsar service
      - "8080:8080"  # Admin API
    command: bin/pulsar standalone
    volumes:
      - pulsardata:/pulsar/data
      - pulsarconf:/pulsar/conf
    healthcheck:
      test: ["CMD", "bin/pulsar-admin", "brokers", "healthcheck"]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - pulsar-network

  pulsar-express:
    image: bbonnin/pulsar-express:latest
    container_name: pulsar-express
    ports:
      - "3000:3000"  # Web UI
    depends_on:
      - pulsar
    environment:
      - PE_CONNECTION_URL=http://pulsar:8080
    networks:
      - pulsar-network

  pyspark:
    image: apache/spark-py:latest
    container_name: pyspark-worker
    user: root
    ports:
      - "4040:4040"  # Spark UI
    depends_on:
      pulsar:
        condition: service_healthy
    environment:
      - PULSAR_SERVICE_URL=pulsar://pulsar:6650
    volumes:
      - ./scripts:/opt/spark/scripts
      - ./data:/opt/spark/data
    working_dir: /opt/spark/scripts
    command: >
      bash -c "
        pip install pulsar-client pandas matplotlib seaborn &&
        tail -f /dev/null
      "
    networks:
      - pulsar-network

volumes:
  pulsardata:
  pulsarconf:

networks:
  pulsar-network:
    driver: bridge
```

Start the services:

``` bash
# Create directories
mkdir scripts data

# Start containers
docker-compose up -d

# Check status
docker-compose ps
```

You should see: - Pulsar: http://localhost:8080 (Admin API) - Pulsar Express: http://localhost:3000 (Web UI) - Spark UI: http://localhost:4040 (when running jobs)

### **Step 2: Generate Dummy Sales Data**

Create `generate_sales_data.py`:

``` python
import pulsar
import json
import random
import time
from datetime import datetime
from faker import Faker

fake = Faker()

# Connect to Pulsar
PULSAR_URL = 'pulsar://localhost:6650'
client = pulsar.Client(PULSAR_URL)
producer = client.create_producer('sales-events')

# Product catalog
PRODUCTS = [
    {"id": 1, "name": "Laptop", "category": "Electronics", "base_price": 1200},
    {"id": 2, "name": "Smartphone", "category": "Electronics", "base_price": 800},
    {"id": 3, "name": "Headphones", "category": "Electronics", "base_price": 150},
    {"id": 4, "name": "Desk Chair", "category": "Furniture", "base_price": 300},
    {"id": 5, "name": "Monitor", "category": "Electronics", "base_price": 400},
]

REGIONS = ["North America", "Europe", "Asia", "South America"]
PAYMENT_METHODS = ["Credit Card", "Debit Card", "PayPal", "Bank Transfer"]

def generate_sale_event():
    """Generate a realistic sales transaction"""
    product = random.choice(PRODUCTS)
    quantity = random.randint(1, 5)
    price = product["base_price"] * random.uniform(0.9, 1.1)
    
    event = {
        "event_id": fake.uuid4(),
        "timestamp": datetime.now().isoformat(),
        "customer_id": fake.uuid4(),
        "customer_name": fake.name(),
        "product_id": product["id"],
        "product_name": product["name"],
        "category": product["category"],
        "quantity": quantity,
        "price": round(price, 2),
        "total_amount": round(price * quantity, 2),
        "region": random.choice(REGIONS),
        "payment_method": random.choice(PAYMENT_METHODS),
        "discount": round(random.uniform(0, 0.2), 2)
    }
    
    return event

def main():
    print("üöÄ Starting Sales Data Generator")
    print(f"üì° Connected to Pulsar at {PULSAR_URL}")
    print(f"üì§ Sending to topic: sales-events")
    print("=" * 60)
    
    try:
        event_count = 0
        while True:
            event = generate_sale_event()
            producer.send(json.dumps(event).encode('utf-8'))
            
            event_count += 1
            print(f"‚úì Event {event_count}: {event['customer_name']} bought "
                  f"{event['quantity']}x {event['product_name']} - "
                  f"${event['total_amount']:.2f}")
            
            time.sleep(random.uniform(0.5, 2))
            
    except KeyboardInterrupt:
        print(f"\n\nüìä Total events sent: {event_count}")
        print("üëã Shutting down gracefully...")
    finally:
        producer.close()
        client.close()
        print("‚úÖ Connection closed")

if __name__ == "__main__":
    main()
```

Install dependencies and run:

``` bash
pip install pulsar-client faker
python generate_sales_data.py
```

Let it run for a minute to generate \~50-100 events, then press Ctrl+C.

### **Step 3: Process with PySpark**

Create `scripts/analyze_sales.py`:

``` python
import os
import pulsar
import json
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *

# Connect to Pulsar
PULSAR_URL = os.getenv('PULSAR_SERVICE_URL', 'pulsar://pulsar:6650')
client = pulsar.Client(PULSAR_URL)
consumer = client.subscribe('sales-events', 'spark-analytics')

# Consume messages
print("üì• Consuming messages from Pulsar...\n")
messages = []

for i in range(100):
    try:
        msg = consumer.receive(timeout_millis=3000)
        data = json.loads(msg.data().decode('utf-8'))
        messages.append(data)
        consumer.acknowledge(msg)
        
        if (i + 1) % 10 == 0:
            print(f"  ‚úì Consumed {i + 1} messages")
    except:
        break

print(f"\n‚úÖ Total messages consumed: {len(messages)}")

# Create Spark session
spark = SparkSession.builder \
    .appName("Pulsar Sales Analytics") \
    .master("local[*]") \
    .getOrCreate()

# Define schema
schema = StructType([
    StructField("event_id", StringType(), True),
    StructField("timestamp", StringType(), True),
    StructField("customer_name", StringType(), True),
    StructField("product_name", StringType(), True),
    StructField("category", StringType(), True),
    StructField("quantity", IntegerType(), True),
    StructField("price", DoubleType(), True),
    StructField("total_amount", DoubleType(), True),
    StructField("region", StringType(), True),
    StructField("payment_method", StringType(), True),
])

# Create DataFrame
df = spark.createDataFrame(messages, schema=schema)

print("\n" + "="*60)
print("üí∞ SALES BY CATEGORY")
print("="*60)

sales_by_category = df.groupBy('category') \
    .agg(
        sum('total_amount').alias('total_sales'),
        sum('quantity').alias('total_quantity'),
        count('*').alias('num_transactions')
    ) \
    .orderBy(col('total_sales').desc())

sales_by_category.show()

print("\n" + "="*60)
print("üåç SALES BY REGION")
print("="*60)

sales_by_region = df.groupBy('region') \
    .agg(
        sum('total_amount').alias('total_sales'),
        count('*').alias('num_transactions')
    ) \
    .orderBy(col('total_sales').desc())

sales_by_region.show()

# Save results
sales_by_category.toPandas().to_csv("/opt/spark/data/sales_by_category.csv", index=False)
sales_by_region.toPandas().to_csv("/opt/spark/data/sales_by_region.csv", index=False)

print("\n‚úÖ Results saved to CSV files")

consumer.close()
client.close()
spark.stop()
```

Run the analysis:

``` bash
docker exec pyspark-worker python /opt/spark/scripts/analyze_sales.py
```

------------------------------------------------------------------------

## Code Explanation: Deep Dive

Let's break down what's happening in our pipeline:

### **Producer Side (generate_sales_data.py)**

``` python
client = pulsar.Client('pulsar://localhost:6650')
producer = client.create_producer('sales-events')
```

**What's happening:** 1. We create a Pulsar client connection 2. We create a producer for the `sales-events` topic 3. Pulsar automatically creates the topic if it doesn't exist

``` python
producer.send(json.dumps(event).encode('utf-8'))
```

**Key points:** - Messages must be bytes, so we JSON-encode and convert to UTF-8 - `send()` is synchronous by default‚Äîit waits for broker acknowledgment - For async: `producer.send_async(message, callback=my_callback)`

### **Consumer Side (analyze_sales.py)**

``` python
consumer = client.subscribe('sales-events', 'spark-analytics')
```

**What's happening:** 1. We subscribe to the `sales-events` topic 2. `'spark-analytics'` is our subscription name 3. Pulsar tracks our position in the topic per subscription 4. Multiple consumers with the same subscription share the load

``` python
msg = consumer.receive(timeout_millis=3000)
data = json.loads(msg.data().decode('utf-8'))
consumer.acknowledge(msg)
```

**Message lifecycle:** 1. `receive()` blocks until a message arrives or timeout 2. We decode bytes ‚Üí string ‚Üí JSON 3. `acknowledge()` tells Pulsar "I've processed this successfully" 4. If we crash before acknowledging, Pulsar redelivers the message

### **Spark Processing**

``` python
df = spark.createDataFrame(messages, schema=schema)

sales_by_category = df.groupBy('category') \
    .agg(
        sum('total_amount').alias('total_sales'),
        count('*').alias('num_transactions')
    )
```

**Why Spark:** - Handles large-scale data processing - Distributed computing (can scale to clusters) - Rich transformation and aggregation API - Integrates well with data lakes and warehouses

------------------------------------------------------------------------

## Monitoring Your Pipeline

### **Pulsar Express UI** (http://localhost:3000)

View: - Active topics and message rates - Consumer lag (how far behind consumers are) - Subscription status - Storage usage

### **Spark UI** (http://localhost:4040)

When running Spark jobs, monitor: - Job execution timeline - Stage breakdown - Memory usage - Task distribution

### **Admin API** (http://localhost:8080)

Check topic stats:

``` bash
curl http://localhost:8080/admin/v2/persistent/public/default/sales-events/stats
```

------------------------------------------------------------------------

## Production Perspective: What's Next?

### **Scaling Considerations**

**Horizontal Scaling:** - Add more Pulsar brokers for higher throughput - Add more BookKeeper nodes for storage capacity - Deploy multiple consumer instances for parallel processing

**Partitioning:**

``` python
# Create partitioned topic for better distribution
producer = client.create_producer(
    'sales-events',
    partitions=10  # Split across 10 partitions
)
```

**Message Routing:**

``` python
# Ensure all events from same customer go to same partition
producer.send(
    message,
    partition_key=customer_id  # Route by customer
)
```

### **Reliability Patterns**

**Guaranteed Delivery:**

``` python
producer = client.create_producer(
    'sales-events',
    producer_name='sales-producer-1',
    batching_enabled=True,
    block_if_queue_full=True
)
```

**Idempotent Consumers:**

``` python
# Track processed event IDs to handle duplicates
processed_ids = set()

msg = consumer.receive()
event_id = json.loads(msg.data())['event_id']

if event_id not in processed_ids:
    process_event(msg.data())
    processed_ids.add(event_id)
    consumer.acknowledge(msg)
```

**Dead Letter Topics:**

``` python
consumer = client.subscribe(
    'sales-events',
    'my-subscription',
    dead_letter_policy=pulsar.ConsumerDeadLetterPolicy(
        max_redeliver_count=3,
        dead_letter_topic='sales-events-dlq'
    )
)
```

### **Real-World Use Cases**

**E-commerce:** - Order processing pipelines - Inventory updates - Real-time analytics dashboards - Recommendation engines

**IoT & Sensor Data:** - Device telemetry streams - Anomaly detection - Predictive maintenance - Time-series analytics

**Financial Services:** - Transaction processing - Fraud detection - Market data feeds - Risk calculations

**Social Media:** - Activity feeds - Content moderation - Engagement analytics - Notification systems

------------------------------------------------------------------------

## Key Takeaways

**Why Choose Pulsar?**

1.  **Unified messaging**: Queueing + streaming in one platform
2.  **Cloud-native**: Built for Kubernetes and multi-tenant deployments
3.  **Geo-replication**: Built-in cross-datacenter replication
4.  **Storage separation**: Independent scaling of compute and storage
5.  **Multi-language**: Clients for Java, Python, Go, C++, Node.js, and more

**When to Use Pulsar:**

-   ‚úÖ Need both pub-sub and queuing semantics
-   ‚úÖ Require strong ordering guarantees
-   ‚úÖ Multi-tenancy is important
-   ‚úÖ Planning for global distribution
-   ‚úÖ Need durability with low latency

**When to Consider Alternatives:**

-   If you only need simple pub-sub ‚Üí NATS
-   If you need complex stream processing ‚Üí Kafka with Kafka Streams
-   If you want managed service ‚Üí AWS Kinesis, Google Pub/Sub

------------------------------------------------------------------------

## Conclusion

Apache Pulsar represents a new generation of messaging platforms‚Äîcombining the best of message queues and streaming systems while adding cloud-native features like multi-tenancy and geo-replication. By thinking of it as a modern postal service, we can understand how producers, consumers, topics, and subscriptions work together to deliver messages reliably at scale.

Our hands-on demo showed how easy it is to build a real-time analytics pipeline: generating events, streaming through Pulsar, processing with PySpark, and analyzing results. This same pattern scales from development laptops to production clusters handling millions of messages per second.

Whether you're building microservices, processing IoT data, or creating real-time analytics dashboards, Pulsar provides the reliable, scalable foundation you need.

Ready to dive deeper? Check out the [official Pulsar documentation](https://pulsar.apache.org/docs/) and join the vibrant community!

------------------------------------------------------------------------

## Further Reading

-   [Apache Pulsar Documentation](https://pulsar.apache.org/docs/)
-   [Pulsar GitHub Repository](https://github.com/apache/pulsar)
-   [Pulsar Community Slack](https://pulsar.apache.org/community/)
-   [Pulsar Summit](https://pulsar.apache.org/community/#events)

**Try it yourself:** All code from this tutorial is available in the demo files above. Clone, experiment, and build something amazing!

------------------------------------------------------------------------

*Have questions or built something cool with Pulsar? Share your experience in the comments below!*